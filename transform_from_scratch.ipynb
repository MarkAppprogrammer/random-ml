{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoR18ens371MVd1ge+zYLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkAppprogrammer/random-ml/blob/main/transform_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "90jiwvCe9oGI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import torch\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from zeugma.embeddings import EmbeddingTransformer\n",
        "\n",
        "#import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization + Embedding\n",
        "Tried to use openai's encoder but looking again at the paper all that is needed is to break it into words and embed using word2vec. Then postional encoding."
      ],
      "metadata": {
        "id": "ZAb7jera9yX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization and Embedding\n",
        "sample_input = \"The dark brown fox jumped over the small road\"\n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(sample_input)\n",
        "\n",
        "# cl100k_tokenizer = openai.cl100k_base()\n",
        "# tokens = cl100k_tokenizer.encode(sample_input)"
      ],
      "metadata": {
        "id": "tyBa0CMn9yv_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('brown')\n",
        "nltk.download('word2vec_sample')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS_FV_uCb33R",
        "outputId": "ad0d3305-ce77-4792-e2c1-2915b74387b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = Word2Vec(sentences=common_texts, vector_size=512, window=5, min_count=1, workers=4)\n",
        "# train_set = brown.sents()[:10000]\n",
        "# model = Word2Vec(train_set, vector_size=512)"
      ],
      "metadata": {
        "id": "poWV4VKbErGf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.data import find\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
      ],
      "metadata": {
        "id": "JW5VmInLb-aH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(sentences, vector_size=100, window=5, min_count=1, epochs=10):\n",
        "    # Train Word2Vec model\n",
        "    model = Word2Vec(\n",
        "        sentences=sentences,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        sg=1,\n",
        "        workers=4,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "4qKyjik2D9yi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data):\n",
        "   return nltk.tokenize.word_tokenize(sample_input)\n",
        "\n",
        "# note doesnt work with puncation\n",
        "def embedder(tokens):\n",
        "    model = train_word2vec([tokens], vector_size=512)\n",
        "\n",
        "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
        "\n",
        "    return np.array(vectors)"
      ],
      "metadata": {
        "id": "yN6NoR_ZAZ9c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "embedding = embedder(tokens)"
      ],
      "metadata": {
        "id": "io7oJ2kUCV2X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Postional Encoding\n",
        "Note formulas in paper. For this example we use the sample sentence which was previously tokenized and we found mebeddings for."
      ],
      "metadata": {
        "id": "FNIfTyrPdEep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getPositionEncoding(seq_len, d, n=10000):\n",
        "    P = np.zeros((seq_len, d))\n",
        "    for k in range(seq_len):\n",
        "        for i in np.arange(int(d/2)):\n",
        "            denominator = np.power(n, 2*i/d)\n",
        "            P[k, 2*i] = np.sin(k/denominator)\n",
        "            P[k, 2*i+1] = np.cos(k/denominator) #forumulae from paper\n",
        "    return P\n",
        "\n",
        "#example\n",
        "P = getPositionEncoding(9, 512, 10000)\n",
        "embedding_with_pos = embedding + P"
      ],
      "metadata": {
        "id": "emlxk062c33b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attenion Mechainsms\n",
        "\n",
        "We need to make the scaled dot-product addtion and multi-head addditon used in the encoder and decoder stack of the transformer."
      ],
      "metadata": {
        "id": "1JzrgGttuPTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def attention(Q, K, V):\n",
        "#   return np.matmul(softmax((np.matmul(Q, K) * (1/np.sqrt(K.shape[1]))), axis=0), V)\n",
        "\n",
        "def attention(Q, K, V):\n",
        "    d_k = K.shape[-1]\n",
        "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
        "    weights = softmax(scores, axis=-1)\n",
        "    return np.matmul(weights, V)"
      ],
      "metadata": {
        "id": "GhITLWgouDQK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glorotInit(shape):\n",
        "    fan_in, fan_out = shape[0], shape[1]\n",
        "    limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "    return np.random.uniform(-limit, limit, size=shape)\n",
        "\n",
        "def initWeights(d_model, d_k, d_v, h):\n",
        "  # q, k, and v wegiths need to also have for all i shape should be (d_model, d_k, h) for W_Q\n",
        "  W_Q = [glorotInit((d_model, d_k)) for _ in range(h)]\n",
        "  W_K = [glorotInit((d_model, d_k)) for _ in range(h)]\n",
        "  W_V = [glorotInit((d_model, d_v)) for _ in range(h)]\n",
        "  W_O = glorotInit((h * d_v, d_model))\n",
        "\n",
        "  return W_Q, W_K, W_V, W_O\n",
        "\n",
        "def multiHeadAttention(Q, K, V, h, d_model):\n",
        "  W_Q, W_K, W_V, W_O = initWeights(d_model, K.shape[1], V.shape[1], h)\n",
        "\n",
        "  heads = []\n",
        "  for i in range(h):\n",
        "        Q_proj = np.matmul(Q, W_Q[i])\n",
        "        K_proj = np.matmul(K, W_K[i])\n",
        "        V_proj = np.matmul(V, W_V[i])\n",
        "\n",
        "        heads.append(attention(Q_proj, K_proj, V_proj))\n",
        "\n",
        "  concat = np.concatenate(heads, axis=-1)\n",
        "  return np.matmul(concat, W_O)"
      ],
      "metadata": {
        "id": "xVLFl7iewMfY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and Decoder Stacks\n",
        "\n",
        "Now its time to combine everything we made above into the encoder and decoder stacks"
      ],
      "metadata": {
        "id": "SX3GFtwg_-qS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zgc-EDzWAFUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}